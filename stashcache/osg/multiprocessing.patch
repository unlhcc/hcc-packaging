From 885a28cbe57c0467afcb64e403b1acb9ddd49378 Mon Sep 17 00:00:00 2001
From: John Thiltges <jthiltges2@unl.edu>
Date: Fri, 5 Oct 2018 16:03:20 -0500
Subject: [PATCH 1/3] Switch stashcache stats collection to multiprocessing and
 cache results

- Walking the cache filesystem to collect stats may take more than 15 minutes
  to complete
- Condor expires ads unless they are refreshed at least every 15 minutes
- Cache the results for 2 hours and keep advertising <= 15 minutes
- Threading didn't work due to setuid/gid calls in condor libraries
    - Condor's advertise changes the process uid/gid, which causes
      running stats collections to fail with permissions errors
    - Multiprocessing lets the collection run in a separate uid/gid space
- Add `--collectors` and `--cache-walk-interval` as CLI options
---
 src/stashcache | 183 ++++++++++++++++++++++++++++++++++++++++---------
 1 file changed, 149 insertions(+), 34 deletions(-)

diff --git a/src/stashcache b/src/stashcache
index ef7cc3a..c5fc068 100755
--- a/src/stashcache
+++ b/src/stashcache
@@ -1,59 +1,149 @@
 #!/bin/env python
 '''
 Collect XRootD stats and report to HTCondor collector
+
+- StashCache reporter is called periodically from cron or timer
+    - It advertises the usage data as a Condor ad to an OSG collector
+    - Runs every 5 minutes (since the ad expires after 15 min)
+- Reporter maintains a cache of the most recently collected usage data
+    - Saved to /tmp as JSON
+- If the cached usage data is current (less than two hours old), the reporter
+  advertises the cached data and exits
+- If the cached usage data is old, the reporter updates the cache, and also
+  runs a process to advertise the most recent cached data every ~5 minutes.
+  Even if the stats collection takes over 15 minutes, the ad won't expire from
+  the collector.
 '''
 
 # Viewing stats in collector:
 # condor_status -pool collector1.opensciencegrid.org:9619 -any xrootd@hcc-stash.unl.edu -l
 
 import argparse
+import json
 import logging
 import os
+import multiprocessing
 import random
 import sys
 import socket
+import tempfile
 import time
 
+import classad
 import htcondor
 import xrootd_cache_stats
 
 # Ad expires from collector after 15 minutes
-REFRESH_INTERVAL = 600 + random.randrange(180) # seconds
-
-def heart_beat(hostname, cache_path='/stash', one_shot=False):
-    '''Send heartbeats and cache ads to the condor_master'''
-    try:
-        central_coll_url = htcondor.param['OSG_COLLECTOR_HOST']
-    except KeyError:
-        central_coll_url = 'collector1.opensciencegrid.org:9619,collector2.opensciencegrid.org:9619'
-    logging.debug('Using %s as the central collector', central_coll_url)
-    central_coll = htcondor.Collector(central_coll_url)
-
-    xrootd_url = 'root://' + hostname
-    while True:
+AD_REFRESH_INTERVAL = 600 + random.randrange(180) # seconds
+
+# URLs of central OSG collectors (used if OSG_COLLECTOR_HOST not defined)
+CENTRAL_COLLECTORS = 'collector1.opensciencegrid.org:9619,collector2.opensciencegrid.org:9619'
+
+# StashCache version (set at build time)
+STASHCACHE_VERSION = '##VERSION##'
+
+class StashCacheReporter(object):
+    '''Collect XRootD stats, cache state for performance, and report to collector'''
+    def __init__(self, cache_path='/stash', collectors=CENTRAL_COLLECTORS,
+                 cache_walk_interval=2*60*60, log_level=30):
+        self.logger = logging.getLogger(__name__)
+        self.logger.setLevel(log_level)
+        self.cache_path = cache_path
+        self.state_file = self.get_state_filename(cache_path)
+        self.collectors = collectors
+        self.cache_walk_interval = cache_walk_interval
+
+        manager = multiprocessing.Manager()
+        self.state = manager.dict()
+
+        self.load_state()
+
+    @staticmethod
+    def get_state_filename(cache_path):
+        '''Generate state filename for cache_path:
+        - Replace non-alphanumeric characters in cache_path with dash
+        - Prepend temporary directory'''
+        state_file = 'stashcache-reporter'
+        state_file += ''.join(c if c.isalnum() else '.' for c in cache_path)
+        state_file += '.json'
+        return os.path.join(tempfile.gettempdir(), state_file)
+
+    def load_state(self):
+        '''Load previous run's state from JSON'''
+        try:
+            with open(self.state_file, 'rb') as fptr:
+                self.state.update(json.load(fptr))
+                self.logger.debug('Loaded cache state from %s', self.state_file)
+                return True
+        except IOError as err:
+            self.logger.debug('Could not open cache state file %s: %s', self.state_file, err)
+        except ValueError as err:
+            self.logger.warning('Could not parse cache state file %s: %s', self.state_file, err)
+
+        return False
+
+    def save_state(self):
+        '''Save current run's state to JSON'''
+        with open(self.state_file, 'wb') as fptr:
+            json.dump(dict(self.state), fptr)
+            self.logger.debug('Wrote cache state for %s to %s', self.cache_path, self.state_file)
+
+    def stat_collector(self):
+        '''Advertise stats, and walk cache to update stats if expired'''
+        self.advertise_cache_stats()
+
+        if self.state.get('last_scan', 0) + self.cache_walk_interval < time.time():
+            self.walk_cache()
+            self.advertise_cache_stats()
+        else:
+            self.logger.debug('Skipping cache stat collection, state not expired')
+
+    def walk_cache(self):
+        '''Walk cache directory to collect stats and update state'''
+        xrootd_url = 'root://' + socket.getfqdn()
+
+        self.logger.debug('Collecting cache stats from %s', self.cache_path)
         start_time = time.time()
-        logging.debug('Collecting cache stats from %s', cache_path)
-        cache_ad = xrootd_cache_stats.collect_cache_stats(xrootd_url, cache_path)
-        cache_ad['STASHCACHE_DaemonVersion'] = '##VERSION##'
+        cache_ad = xrootd_cache_stats.collect_cache_stats(xrootd_url, self.cache_path)
+        cache_ad['STASHCACHE_DaemonVersion'] = STASHCACHE_VERSION
+        end_time = time.time()
+        self.logger.info('Cache stat collection for %s took %.2f seconds', self.cache_path,
+                         end_time - start_time)
+
         if cache_ad['ping_response_status'] == 'ok':
-            logging.debug('XRootD server (%s) status: OK', xrootd_url)
-            pool = central_coll.query(htcondor.AdTypes.Collector)[0]['Machine']
-            logging.info('Advertising StashCache ads to %s', pool)
-            # Save and restore euid, as advertise() changes it
-            old_uid = os.geteuid()
-            central_coll.advertise([cache_ad], 'UPDATE_STARTD_AD')
-            os.seteuid(old_uid)
+            self.logger.debug('XRootD server (%s) status: OK', xrootd_url)
+            # json can't serialize classads. Need to convert them to/from str.
+            self.state['cache_ad'] = str(cache_ad)
+            self.state['last_scan'] = end_time
+            self.save_state()
         else:
             logging.warning('No heartbeat from XRootD server')
 
-        if one_shot:
-            break
-        else:
-            end_time = time.time()
-            logging.debug('Cache stat collection took %s seconds', end_time - start_time)
-            sleep_time = max(0, REFRESH_INTERVAL - (end_time - start_time))
-            logging.debug('Sleeping %d seconds before ad refresh', sleep_time)
-            time.sleep(sleep_time)
+    def advertise_cache_stats(self):
+        '''Send cache ad to collector'''
+        if 'cache_ad' not in self.state:
+            return False
+
+        # classad cannot parse unicode
+        cache_ad_txt = self.state['cache_ad'].encode('ascii', 'ignore')
+        cache_ad = classad.parseOne(cache_ad_txt)
+
+        coll = htcondor.Collector(self.collectors)
+
+        self.logger.info('Advertising StashCache ads to collectors: %s', self.collectors)
+        # Save and restore euid, as advertise() changes it
+        old_uid = os.geteuid()
+        coll.advertise([cache_ad], 'UPDATE_STARTD_AD')
+        os.seteuid(old_uid)
+
+        return True
+
+    def advertiser_loop(self):
+        '''If we have a cache ad, loop forever, sending cache ad to collector'''
+        while True:
+            self.logger.debug('Sleeping %d seconds before ad refresh', AD_REFRESH_INTERVAL)
+            time.sleep(AD_REFRESH_INTERVAL)
+            self.advertise_cache_stats()
 
 def main():
     '''Main function'''
@@ -71,9 +161,28 @@ def main():
             logging.error('Could not find host %s at %s', pki, pki_path)
             sys.exit(1)
 
-    # Monitor the xrootd service
-    hostname = socket.getfqdn()
-    heart_beat(hostname, one_shot=args.one_shot, cache_path=args.cache_path)
+    scr = StashCacheReporter(cache_path=args.cache_path, collectors=args.collectors,
+                             cache_walk_interval=args.cache_walk_interval,
+                             log_level=log_level)
+
+    # Periodically advertise stats while collector runs
+    ad_reporter = multiprocessing.Process(target=scr.advertiser_loop)
+    ad_reporter.start()
+
+    while True:
+        # Start collecting stats and wait for it to finish
+        stat_collector = multiprocessing.Process(target=scr.stat_collector)
+        stat_collector.start()
+        stat_collector.join()
+
+        if args.one_shot:
+            break
+        else:
+            # Sleep before refreshing cache, but keep advertising
+            logging.debug('Sleeping %d seconds before cache refresh', args.cache_walk_interval)
+            time.sleep(args.cache_walk_interval)
+
+    ad_reporter.terminate()
 
 def parse_args():
     '''Parse CLI options'''
@@ -83,6 +192,12 @@ def parse_args():
                         help='Run once, rather than persistently')
     parser.add_argument('--cache-path', default='/stash',
                         help='Path to the local XRootD stashcache directory')
+    parser.add_argument('--collectors',
+                        default=htcondor.param.get('OSG_COLLECTOR_HOST', CENTRAL_COLLECTORS),
+                        help='List of HTCondor collectors to receive ads')
+    parser.add_argument('--cache-walk-interval', type=int,
+                        default=2*60*60, # 2 hours
+                        help='Minimum seconds between walking the cache to refresh stats')
     parser.add_argument('-v', '--verbose', dest='verbose_count',
                         action='count', default=0,
                         help='Increase log verbosity (repeatable)')
-- 
2.17.1


From b7fc8382d219cb4b71808d6d3bcc8fc7cdf1a9ce Mon Sep 17 00:00:00 2001
From: John Thiltges <jthiltges2@unl.edu>
Date: Wed, 10 Oct 2018 10:04:38 -0500
Subject: [PATCH 2/3] Forking (multiprocessing) requires XRootD fork handler

---
 src/stashcache | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/stashcache b/src/stashcache
index c5fc068..df1d7c1 100755
--- a/src/stashcache
+++ b/src/stashcache
@@ -29,6 +29,10 @@ import socket
 import tempfile
 import time
 
+# multiprocessing requires XRootD fork handler
+# Must set before importing XRootD.client
+os.putenv('XRD_RUNFORKHANDLER', '1')
+
 import classad
 import htcondor
 import xrootd_cache_stats
-- 
2.17.1


From 1cca45709ae9754bd113d93bc94b1a9d2229ac88 Mon Sep 17 00:00:00 2001
From: John Thiltges <jthiltges2@unl.edu>
Date: Wed, 10 Oct 2018 10:09:23 -0500
Subject: [PATCH 3/3] Update cinfo parser to file format v2

---
 src/xrootd_cache_stats.py | 71 +++++++++++++++++++++------------------
 1 file changed, 38 insertions(+), 33 deletions(-)

diff --git a/src/xrootd_cache_stats.py b/src/xrootd_cache_stats.py
index 485d288..dd28530 100755
--- a/src/xrootd_cache_stats.py
+++ b/src/xrootd_cache_stats.py
@@ -5,6 +5,7 @@ that can be handed to condor
 """
 
 import os
+import math
 import time
 import errno
 import struct
@@ -111,7 +112,7 @@ def scan_vo_dir(vodir):
 
             for h in access_info["by_hour"]:
                 accesses["naccesses_hr_" + h] += access_info["by_hour"][h]
-                accesses["bytes_hr_" + h] += access_info["by_hour"][h]*file_size
+                accesses["bytes_hr_" + h] += access_info["bytes_hr"][h]
 
     result = classad.ClassAd({
                             "used_bytes" : totalsize,
@@ -128,20 +129,20 @@ def scan_vo_dir(vodir):
 # Parsing the cinfo files
 
 # The header (not a c struct; consecutive separate values with no padding)
-# version + buffer size + download status array size (bits) + download status array
-#   int   +  long long  +              int                  +       variable
-_header_fmt = '=iqi'
+# version + buffer size + file size (blocks)
+# int     + long long   + long long
+_header_fmt = '=iqq'
 _header_fmt_size = struct.calcsize(_header_fmt)
 
 # then the number of accesses
 #   int
-_int_fmt = '@i'
+_int_fmt = '@q'
 _int_fmt_size = struct.calcsize(_int_fmt)
 
 # each access contains a struct (native size + padding)
-# detach time + bytes disk + bytes ram + bytes missed
-# time_t      + long long  + long long + long long
-_status_fmt = '@lqqq'
+# AttachTime + DetachTime + BytesDisk + BytesRam  + BytesMissed
+# time_t     + long long  + long long + long long + long long
+_status_fmt = '@qqqqq'
 _status_fmt_size = struct.calcsize(_status_fmt)
 
 class ReadCInfoError(Exception):
@@ -158,6 +159,7 @@ def read_cinfo(cinfo_file, now):
     result = { "naccesses": 0,
                "last_access": 0,
                "by_hour" : { "01": 0, "12": 0, "24": 0 },
+               "bytes_hr" : { "01": 0, "12": 0, "24": 0 },
              }
 
     cf = open(cinfo_file, 'rb')
@@ -168,15 +170,20 @@ def read_cinfo(cinfo_file, now):
         # a mangled file
         raise ReadCInfoError("%s header too short" % cinfo_file, result)
 
-    version, buffer_size, status_array_size_bits = struct.unpack(_header_fmt, buf)
+    version, buffer_size, file_size = struct.unpack(_header_fmt, buf)
 
-    # we only understand version 0
-    if version != 0:
+    # we only understand version 2
+    if version != 2:
         raise ReadCInfoError("%s unknown version: %s" % (cinfo_file, version), result)
 
-    # get the size of the status array and skip over it
-    status_array_size_bytes = (status_array_size_bits - 1)//8 + 1
-    cf.seek(status_array_size_bytes, os.SEEK_CUR)
+    # Get the size of the state vector and skip over it
+    # buff_synced uses 1 bit per bufferSize block of bytes
+    # Length is rounded up to the nearest byte
+    buff_synced_len = int(math.ceil(float(file_size)/buffer_size/8))
+    cf.read(buff_synced_len)
+
+    # Go past cksum (char[16]) and creationTime (time_t)
+    cf.read(16 + 8)
 
     # now the access count (an int)
     buf = cf.read(_int_fmt_size)
@@ -198,28 +205,26 @@ def read_cinfo(cinfo_file, now):
     hr_12 = now - 12*60*60
     hr_24 = now - 24*60*60
 
-    # seek to the most recent access and work backwards
-    start_pos = cf.tell() # don't go before this
-
+    # Read AStat structs
     try:
-        cf.seek(start_pos + (access_count-1)*_status_fmt_size, os.SEEK_SET)
-        buf = cf.read(_status_fmt_size)
-        access_time, _, _, _ = struct.unpack(_status_fmt, buf)
-        result["last_access"] = access_time
-        while True:
-            if access_time >= hr_01: result["by_hour"]["01"] += 1
-            if access_time >= hr_12: result["by_hour"]["12"] += 1
-            if access_time >= hr_24: result["by_hour"]["24"] += 1
+        for buf in iter(lambda: cf.read(_status_fmt_size), b''):
+            access_time, _, bytes_disk, bytes_ram, _ = struct.unpack(_status_fmt, buf)
+            result["last_access"] = access_time
+
+            #print access_time, bytes_disk, bytes_ram
+            #print time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(access_time))
+
+            intervals = list()
+            if access_time >= hr_01: intervals.append('01')
+            if access_time >= hr_12: intervals.append('12')
+            if access_time >= hr_24: intervals.append('24')
             else:
                 # no longer interested
-                break
-
-            cf.seek(-2*_status_fmt_size, os.SEEK_CUR)
-            if cf.tell() < start_pos:
-                # done them all
-                break
-            buf = cf.read(_status_fmt_size)
-            access_time, _, _, _ = struct.unpack(_status_fmt, buf)
+                next
+
+            for interval in intervals:
+                result["by_hour"][interval] += 1
+                result["bytes_hr"][interval] += bytes_disk + bytes_ram
     except struct.error, ex:
         # return what we've got
         raise ReadCInfoError("%s unable to decode access time data: %s" % (cinfo_file, str(ex)), result)
-- 
2.17.1

